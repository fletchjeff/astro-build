"""
fetch_flight_files
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from airflow.models import Variable
from astro import sql as aql
from astro.table import Table, Metadata
import pandas as pd
import pendulum

import pandas
import xgboost
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from smart_open import open
import boto3
import joblib
import datetime

@aql.run_raw_sql(conn_id="snowflake_jeffletcher", task_id="cancelled_flights_count", handler=lambda x: pd.DataFrame(x.fetchall(), columns=x.keys()))
def cancelled_flights_count_func():
    return """
    select count(*) as cancelled_flights from flight_data_2 where cancelled = 1
    """

@aql.dataframe(task_id="df_to_int")
def df_to_int_func(cancelled_flights_count: pd.DataFrame):
    ## here
    return cancelled_flights_count.iloc[0]['cancelled_flights']

@aql.run_raw_sql(conn_id="snowflake_jeffletcher", task_id="fetch_normal_flight_data_sample", handler=lambda x: pd.DataFrame(x.fetchall(), columns=x.keys()))
def fetch_normal_flight_data_sample_func(df_to_int: Table):
    return """
    select FL_DATE, OP_CARRIER, OP_CARRIER_FL_NUM, ORIGIN, DEST, CRS_DEP_TIME, CRS_ARR_TIME, CRS_ELAPSED_TIME, DISTANCE, CANCELLED from flight_data_2 sample ({{df_to_int}} rows) where cancelled = 0
    """

@aql.run_raw_sql(conn_id="snowflake_jeffletcher", task_id="fetch_cancelled_flight_data", handler=lambda x: pd.DataFrame(x.fetchall(), columns=x.keys()))
def fetch_cancelled_flight_data_func():
    return """
    select FL_DATE, OP_CARRIER, OP_CARRIER_FL_NUM, ORIGIN, DEST, CRS_DEP_TIME, CRS_ARR_TIME, CRS_ELAPSED_TIME, DISTANCE, CANCELLED from flight_data_2 where cancelled = 1
    """

@aql.dataframe(task_id="merge_rows")
def merge_rows_func(fetch_cancelled_flight_data: pd.DataFrame, fetch_normal_flight_data_sample: pd.DataFrame):
    print(type(fetch_cancelled_flight_data))
    print(type(fetch_normal_flight_data_sample))
    concat_rows = pd.concat([fetch_normal_flight_data_sample, fetch_cancelled_flight_data])
    print(len(concat_rows))
    return concat_rows

@aql.dataframe(task_id="train_model")
def train_model_func(merge_rows: pd.DataFrame):
    model_data_clean = merge_rows.dropna()
    
    import os
    from airflow.models.variable import Variable
    from joblib import dump, load
    import scipy
    
    model_dir = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    
    os.environ['AWS_SECRET_ACCESS_KEY'] = Variable.get('AWS_SECRET_ACCESS_KEY')
    os.environ['AWS_ACCESS_KEY_ID'] = Variable.get('AWS_ACCESS_KEY_ID')
    
    print(model_data_clean.columns)
    
    X = model_data_clean[[
        'op_carrier',
        'op_carrier_fl_num',
        'origin',
        'dest',
        'crs_elapsed_time',
        'distance',
    ]]
    
    y = model_data_clean[['cancelled']]
    
    categorical_cols = [
        'op_carrier',
        'op_carrier_fl_num',
        'origin',
        'dest'
    ]
    
    ct = ColumnTransformer(
        [('le', OneHotEncoder(), categorical_cols)],
        remainder='passthrough'
    )
    
    X_trans = ct.fit_transform(X)
    print(type(X_trans))
    print(X_trans.shape)
    
    
    X_train, X_test, y_train, y_test = train_test_split(X_trans, y, random_state=42)
    xgbclf = xgboost.XGBClassifier() 
    
    pipe = Pipeline([('scaler', StandardScaler(with_mean=False)),
            ('xgbclf', xgbclf)])
    
    pipe.fit(X_train, y_train)
    
    client = boto3.client('s3')
    with open(f"s3://jf-ml-data/models/{model_dir}/pipe.joblib",'wb',transport_params={'client': client}) as f:
        dump(pipe,f)
    
    with open(f's3://jf-ml-data/models/{model_dir}/X_test.npz','wb',transport_params={'client': client}) as f:
        scipy.sparse.save_npz(f, X_test)
    
    with open(f's3://jf-ml-data/models/{model_dir}/y_test.parquet.gzip','wb',transport_params={'client': client}) as f:
        y_test.to_parquet(f,compression='gzip')    
    
    #test_score = pipe.score(X_test, y_test)
    return model_dir
    #return X_test
    

@aql.dataframe(task_id="evaluate_model")
def evaluate_model_func(train_model: pd.DataFrame):
    import os
    from airflow.models.variable import Variable
    from joblib import dump, load
    import scipy
    
    os.environ['AWS_SECRET_ACCESS_KEY'] = Variable.get('AWS_SECRET_ACCESS_KEY')
    os.environ['AWS_ACCESS_KEY_ID'] = Variable.get('AWS_ACCESS_KEY_ID')
    
    model_dir = train_model
    
    client = boto3.client('s3')
    with open (f's3://jf-ml-data/models/{model_dir}/X_test.npz','rb',transport_params={'client': client}) as f:
        X_test = scipy.sparse.load_npz(f)
    
    with open (f's3://jf-ml-data/models/{model_dir}/y_test.parquet.gzip','rb',transport_params={'client': client}) as f:
        y_test = pd.read_parquet(f)
    
    with open (f's3://jf-ml-data/models/{model_dir}/pipe.joblib','rb',transport_params={'client': client}) as f:
        pipe = load(f)              
    
    test_score = pipe.score(X_test, y_test)
    
    return test_score

@dag(
    schedule_interval="0 0 * * MON",
    start_date=pendulum.from_format("2022-11-02", "YYYY-MM-DD").in_tz("Europe/Amsterdam"),
)
def fetch_flight_files():
    fetch_cancelled_flight_data = fetch_cancelled_flight_data_func()

    cancelled_flights_count = cancelled_flights_count_func()

    df_to_int = df_to_int_func(
        cancelled_flights_count,
    )

    fetch_normal_flight_data_sample = fetch_normal_flight_data_sample_func(
        df_to_int,
    )

    merge_rows = merge_rows_func(
        fetch_cancelled_flight_data,
        fetch_normal_flight_data_sample,
    )

    train_model = train_model_func(
        merge_rows,
    )

    evaluate_model = evaluate_model_func(
        train_model,
    )

    df_to_int << cancelled_flights_count

    evaluate_model << train_model

    fetch_normal_flight_data_sample << [df_to_int, cancelled_flights_count]

    merge_rows << [fetch_cancelled_flight_data, fetch_normal_flight_data_sample]

    train_model << merge_rows

dag_obj = fetch_flight_files()
